{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # for google colab users\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "  # for local setup\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import itertools as ittl\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA A40\n",
      "NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# package import\n",
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "def run_inference(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    model.to(rank)\n",
    "\n",
    "    if torch.distributed.get_rank() == 0:\n",
    "        prompt = \"a dog\"\n",
    "    elif torch.distributed.get_rank() == 1:\n",
    "        prompt = \"a cat\"\n",
    "\n",
    "    response = model.generate(prompt, max_new_tokens=10)\n",
    "    responses.append(response)\n",
    "    \n",
    "def parallel_inference():\n",
    "    world_size = 2\n",
    "    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jovyan/conda-envs/steering/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/jovyan/conda-envs/steering/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'run_inference' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jovyan/conda-envs/steering/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/jovyan/conda-envs/steering/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'run_inference' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m parallel_inference()\n",
      "\u001b[1;32m/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_inference\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     world_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://lobot.caslab.queensu.ca/home/jovyan/1L-Sparse-Autoencoder/parallel_inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     mp\u001b[39m.\u001b[39;49mspawn(run_inference, args\u001b[39m=\u001b[39;49m(world_size,), nprocs\u001b[39m=\u001b[39;49mworld_size, join\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:246\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    240\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m start_method\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:202\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    201\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    203\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:153\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    146\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with signal \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (error_index, name),\n\u001b[1;32m    147\u001b[0m             error_index\u001b[39m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m             signal_name\u001b[39m=\u001b[39mname,\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    154\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with exit code \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    155\u001b[0m             error_index\u001b[39m=\u001b[39merror_index,\n\u001b[1;32m    156\u001b[0m             error_pid\u001b[39m=\u001b[39mfailed_process\u001b[39m.\u001b[39mpid,\n\u001b[1;32m    157\u001b[0m             exit_code\u001b[39m=\u001b[39mexitcode,\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m original_trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_queues[error_index]\u001b[39m.\u001b[39mget()\n\u001b[1;32m    161\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "parallel_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
